<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>一些对自己未来的胡思乱想</title>
      <link href="/%E4%B8%80%E4%BA%9B%E5%AF%B9%E8%87%AA%E5%B7%B1%E6%9C%AA%E6%9D%A5%E7%9A%84%E8%83%A1%E6%80%9D%E4%B9%B1%E6%83%B3/"/>
      <url>/%E4%B8%80%E4%BA%9B%E5%AF%B9%E8%87%AA%E5%B7%B1%E6%9C%AA%E6%9D%A5%E7%9A%84%E8%83%A1%E6%80%9D%E4%B9%B1%E6%83%B3/</url>
      
        <content type="html"><![CDATA[<h1 id="Saturday-May-10-01-04"><a href="#Saturday-May-10-01-04" class="headerlink" title="Saturday, May 10, 01.04"></a>Saturday, May 10, 01.04</h1><p>最近一直情绪都比较差，也比较不稳定，也很经常有想自杀的想法，甚至一度跑到阳台俯视，真高啊，这要是跳下去肯定很痛，中间或许会后悔吧？</p><p>这也导致我基本上整天的胡思乱想比以前多了很多，刚刚洗澡的时候我在思考，等下洗完澡检索一下，雅思应该怎么复习，筹备一下申请香港博士的计划，幻想着被录取后，年节回家在亲戚面前说我在香港读博，这是多么有光彩，但是刚洗完的时候，脑海中有一个想法突然蹦出来，“这是我自己想去的吗？”<span id="more"></span></p><p>对啊，这是我自己想去的吗？似乎我总是活在了别人的眼里，别人的评价里，即使已经到了25岁的年纪，我想到自己去读博，想到的也是亲戚们赞赏的目光，这真的是我想要的吗？</p><p>于是我回到房间，写下了这篇随记。</p><p>就去香港读博这件事来说，一开始我是完全没有想过这一条路的，直到沐宸说到了或许她可以试试香港那边，这样就算以后我仍旧在中大八院读博，我们之间也不至于距离如此之远。后面慢慢着，我也粗略了解了那边的一些情况，似乎就想到了，我也要过去读博的想法，如果这里深究起来，我觉得可能是因为沐宸的话语，她也提醒了我，如果我也一起过去，我们就能结束异地了，当时我似乎就已经上头，说的对，如果我能去那边，我就能跟女朋友结束异地了，我们甚至还能两个人租一间房，我们布置自己的小家，多好的一件事情啊。随后便跟着她去了解雅思，甚至开始准备雅思，也就到了现在。</p><p>那么，既然现在我意识到了，那么我就需要问问自己，我为什么要过去香港读博，我自己想过去读博吗？我读博为了什么呢？目前看来我之前的想法是，为了和女朋友能结束现在的异地生活，为了能有个博士读，又或许是为了逃避现在我硕士所在的表现不好的地方，以期望换个地方自己就能有更好的表现，又或者是，这样平时回家的时候我就能表现自己？或许有许许多多这种那样的理由吧，但还是回到那个问题，我自己想吗？</p><p>但有时我又会觉得，这本身也是个矛盾循环，那这些理由不能算是我“自己”的理由吗？我去跟Grok聊了聊，他让我不要着急，好好想想问问自己，如果没有上面的或者是我之前想的一些“理由”，我还会去香港吗？同时也坦诚面对自己的逃避，如果逃避是自己的问题，那么或许换个地方也并不能解决问题。其次也好好考虑一下与女朋友的未来，我自己的期望的未来是怎么样的，读博毕竟是个几年的决定，即使上面的这些读博原因某种意义上也能算是“理由”，但这些似乎都是短期的考虑，我需要思考，我的长期动力在哪，这样，我才能在那边遇到困境的时候长久地坚持下去。</p><p>所以说下来，最重要的其实就是，我想要的未来是怎么样的？我需要如何做来奔向我想要的未来，而读博或者说去香港读博是不是能让我更靠近我未来的生活，或许好好思考一下，慢慢我会得出答案。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>搭建医学图像AI常用的基础PyTorch环境</title>
      <link href="/%E6%90%AD%E5%BB%BAPyTorch%E7%8E%AF%E5%A2%83/"/>
      <url>/%E6%90%AD%E5%BB%BAPyTorch%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<h2 id="搭建PyTorch环境"><a href="#搭建PyTorch环境" class="headerlink" title="搭建PyTorch环境"></a>搭建PyTorch环境</h2><h3 id="目标环境"><a href="#目标环境" class="headerlink" title="目标环境"></a>目标环境</h3><ul><li><p><strong>编程语言</strong>：Python（推荐3.8或3.9，稳定且兼容性好）。</p></li><li><p><strong>主要工具</strong></p><ul><li>PyTorch（深度学习模型）。</li><li>scikit-learn（传统机器学习模型）。</li><li>医学影像处理库（如nibabel）。</li></ul></li><li><p><strong>开发工具</strong>：Jupyter Notebook（交互式调试，便于医学数据可视化）。</p></li></ul><h3 id="步骤1：安装基础软件"><a href="#步骤1：安装基础软件" class="headerlink" title="步骤1：安装基础软件"></a>步骤1：安装基础软件</h3><ol><li><h4 id="安装Miniconda（略）"><a href="#安装Miniconda（略）" class="headerlink" title="安装Miniconda（略）"></a>安装Miniconda（略）</h4></li><li><h4 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h4></li></ol><p>打开终端（Windows用Anaconda Prompt），输入：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conda</span> create -n env_name python=<span class="hljs-number">3</span>.<span class="hljs-number">9</span> -y<br></code></pre></td></tr></table></figure><p><code>env_name</code>是环境名字，可自定义。</p><p>激活环境：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">conda <span class="hljs-built_in">activate</span> env_namer<br></code></pre></td></tr></table></figure><p>看到命令行前出现<code>(liver_cancer)</code>表示成功激活。</p><h3 id="步骤2：安装核心库"><a href="#步骤2：安装核心库" class="headerlink" title="步骤2：安装核心库"></a>步骤2：安装核心库</h3><p>在激活的环境下，逐一安装所需库：</p><ol><li><h4 id="更换国内下载源："><a href="#更换国内下载源：" class="headerlink" title="更换国内下载源："></a>更换国内下载源：</h4></li></ol><p>首先清除conda缓存：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">conda clean <span class="hljs-comment">--all</span><br></code></pre></td></tr></table></figure><ul><li><strong>使用清华镜像站Anaconda仓库：</strong></li></ul><p>各系统都可以通过修改用户目录下的 <code>.condarc</code> 文件来使用镜像站。</p><p>不同系统下的 <code>.condarc</code> 目录如下：</p><ul><li>Linux: <code>$&#123;HOME&#125;/.condarc</code></li><li>macOS: <code>$&#123;HOME&#125;/.condarc</code></li><li>Windows: <code>C:\Users\&lt;YourUserName&gt;\.condarc</code></li></ul><p>注：<br>* Windows 用户无法直接创建名为 <code>.condarc</code> 的文件，可先执行 <code>conda config --set show_channel_urls yes</code> 生成该文件之后再修改。<br>* 由于更新过快难以同步，TUNA 等镜像站不同步 <code>pytorch-nightly</code> , <code>pytorch-nightly-cpu</code> , <code>ignite-nightly</code> 这三个包。<br>* 如果您正在从某一镜像源切换到另一镜像源，请检查镜像源是否同步了您所需要的 repo，以及该 repo 是否支持您使用的平台 (e.g. linux-64)。<br>* 为了保证以下配置在所有镜像站可用，配置中只加入了少量必须的第三方源，您可以在下方的列表中自行寻找并添加其他第三方源。</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-params">channels:</span><br>  <span class="hljs-operator">-</span> defaults<br><span class="hljs-params">show_channel_urls:</span> <span class="hljs-literal">true</span><br><span class="hljs-params">default_channels:</span><br>  <span class="hljs-operator">-</span> https:<span class="hljs-symbol">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br>  <span class="hljs-operator">-</span> https:<span class="hljs-symbol">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br>  <span class="hljs-operator">-</span> https:<span class="hljs-symbol">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="hljs-params">custom_channels:</span><br>  <span class="hljs-params">conda-forge:</span> https:<span class="hljs-symbol">//mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br>  <span class="hljs-params">pytorch:</span> https:<span class="hljs-operator">//</span>mirrors.tuna.tsinghua.edu.cn<span class="hljs-operator">/</span>anaconda<span class="hljs-operator">/</span>cloud<br></code></pre></td></tr></table></figure><p>即可添加 Anaconda Python 免费仓库。</p><p>使用下列命令清除索引缓存，并安装常用包测试一下。</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs excel">conda <span class="hljs-built_in">clean</span> -i<br>conda create -<span class="hljs-built_in">n</span> myenv numpy<br></code></pre></td></tr></table></figure><p><strong>使用清华镜像站PyPl软件仓库：</strong><br>设为默认<br>升级 pip 到最新的版本后进行配置：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vim"><span class="hljs-keyword">python</span> -<span class="hljs-keyword">m</span> pip install --upgrade pip<br>pip config <span class="hljs-keyword">set</span> <span class="hljs-keyword">global</span>.<span class="hljs-built_in">index</span>-url https://mirrors.tuna.tsinghua.edu.<span class="hljs-keyword">cn</span>/pypi/web/simple<br></code></pre></td></tr></table></figure><p>如果您到 pip 默认源的网络连接较差，临时使用本镜像站来升级 pip：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">python -m pip install -i https:<span class="hljs-regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="hljs-regexp">/pypi/</span>web/simple --upgrade pip<br></code></pre></td></tr></table></figure><ol start="2"><li><h4 id="安装PyTorch（带GPU支持可选）"><a href="#安装PyTorch（带GPU支持可选）" class="headerlink" title="安装PyTorch（带GPU支持可选）"></a>安装PyTorch（带GPU支持可选）</h4><ul><li><p><strong>用途</strong>：运行各种模型。</p></li><li><p><strong>检查GPU</strong>：如果你有NVIDIA显卡，安装带CUDA支持的版本；否则用CPU版。</p></li><li><p><strong>安装命令</strong>（访问 <a href="https://pytorch.org/get-started/locally/">PyTorch官网</a> 获取最新命令）：</p></li></ul></li></ol><p>​CPU版：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">conda <span class="hljs-keyword">install</span> pytorch torchvision torchaudio cpuonly -c pytorch<br></code></pre></td></tr></table></figure><p>​GPU版（例如CUDA 11.8，需确认你的显卡支持，已经自动安装CUDA，不用匹配系统版本）：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">conda</span> install pytorch torchvision torchaudio pytorch-cuda=<span class="hljs-number">11</span>.<span class="hljs-number">8</span> -c pytorch -c nvidia<br></code></pre></td></tr></table></figure><p>​验证：</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss">python -c &quot;import torch; <span class="hljs-built_in">print</span>(torch.__version__); <span class="hljs-built_in">print</span>(torch.cuda.is_available())&quot;<br></code></pre></td></tr></table></figure><p>​输出示例：<code>2.2.1</code> 和<code> True</code>（GPU可用）或<code>False</code>（CPU）。</p><ol start="3"><li><h4 id="安装其他库"><a href="#安装其他库" class="headerlink" title="安装其他库"></a>安装其他库</h4><p><strong>（1）常用辅助库</strong></p><ul><li><strong>numpy 和 pandas（数据处理）：</strong></li></ul></li></ol><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">conda <span class="hljs-keyword">install</span> numpy pandas -y<br></code></pre></td></tr></table></figure><p>​<strong>matplotlib（可视化）：</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">conda <span class="hljs-keyword">install</span> matplotlib -y<br></code></pre></td></tr></table></figure><p>​<strong>nibabel（读取MRI的NIfTI格式）：</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">conda <span class="hljs-keyword">install</span> -c conda-forge nibabel -y<br></code></pre></td></tr></table></figure><p>​<strong>pydicom（读取Dicom格式）</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">conda <span class="hljs-keyword">install</span> -c conda-forge pydicom -y<br></code></pre></td></tr></table></figure><p>​<strong>opencv（处理常用图像）：</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">conda <span class="hljs-keyword">install</span> -c conda-forge opencv -y<br></code></pre></td></tr></table></figure><p>​<strong>Jupyter Notebook（交互式开发）：</strong></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">conda <span class="hljs-keyword">install </span><span class="hljs-keyword">jupyter </span>-y<br></code></pre></td></tr></table></figure><p>​<strong>（2）其他辅助库或模型库</strong></p><p>​<strong>timm（Vision Transformer支持）：</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> timm -y<br></code></pre></td></tr></table></figure><p>​<strong>scikit-learn（机器学习库）</strong></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">conda <span class="hljs-keyword">install </span><span class="hljs-keyword">scikit-learn </span>-y<br></code></pre></td></tr></table></figure><p>​<strong>XGBoost（梯度提升模型）</strong></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">conda <span class="hljs-keyword">install</span> xgboost -y<br></code></pre></td></tr></table></figure><h3 id="步骤3：验证环境"><a href="#步骤3：验证环境" class="headerlink" title="步骤3：验证环境"></a>步骤3：验证环境</h3><ol><li><h4 id="启动Jupyter-Notebook"><a href="#启动Jupyter-Notebook" class="headerlink" title="启动Jupyter Notebook"></a>启动Jupyter Notebook</h4></li></ol><p>​终端中输入：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">jupyter notebook</span><br></code></pre></td></tr></table></figure><p>​浏览器会自动打开一个界面。</p><ol start="2"><li><h4 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h4></li></ol><p>​在Jupyter中新建一个笔记本，运行以下代码：</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> torch<br><span class="hljs-title">from</span> torchvision <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">import</span> sklearn<br><span class="hljs-keyword">import</span> xgboost<br><span class="hljs-keyword">import</span> nibabel<br><span class="hljs-keyword">import</span> pydicom<br><span class="hljs-keyword">import</span> cv2<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> timm<br><br><span class="hljs-title">print</span>(<span class="hljs-string">&quot;PyTorch version:&quot;</span>, torch.__version__)<br><span class="hljs-title">print</span>(<span class="hljs-string">&quot;scikit-learn version:&quot;</span>, sklearn.__version__)<br><span class="hljs-title">print</span>(<span class="hljs-string">&quot;XGBoost version:&quot;</span>, xgboost.__version__)<br><span class="hljs-title">print</span>(<span class="hljs-string">&quot;All libraries imported successfully!&quot;</span>)<br></code></pre></td></tr></table></figure><p>​如果没有报错，说明环境搭建成功。</p><ol start="3"><li><h4 id="修改jupyter-notebook的默认文件路径"><a href="#修改jupyter-notebook的默认文件路径" class="headerlink" title="修改jupyter notebook的默认文件路径"></a>修改jupyter notebook的默认文件路径</h4><p>运行以下命令，查看配置文件路径：</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs verilog">jupyter notebook --<span class="hljs-keyword">generate</span>-<span class="hljs-keyword">config</span><br></code></pre></td></tr></table></figure><p>输出类似：</p> <figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino">Writing <span class="hljs-keyword">default</span> config to: C:\Users\你的用户名\.jupyter\jupyter_notebook_config.py<br></code></pre></td></tr></table></figure></li></ol><p>​如果文件已存在，会告诉你路径；如果没有，会生成一个默认配置文件。</p><p>​<strong>打开配置文件</strong></p><ul><li>用文本编辑器（如记事本、VS Code）打开上述文件：<ul><li>Windows：C:\Users\你的用户名.jupyter\jupyter_notebook_config.py</li></ul></li></ul><p>​修改默认路径</p><p>在文件中找到这一行（默认被注释）：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-id">#c</span><span class="hljs-selector-class">.ServerApp</span><span class="hljs-selector-class">.notebook_dir</span> = <span class="hljs-string">&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>取消注释（去掉#），并填入你想要的路径，例如：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">c.ServerApp.notebook_dir</span> = <span class="hljs-string">&#x27;C:/project&#x27;</span><br></code></pre></td></tr></table></figure><p><strong>注意</strong>：</p><ul><li>用正斜杠&#x2F;或双反斜杠\（Windows），单反斜杠\可能报错。</li><li>确保路径存在（可以用mkdir创建文件夹）。</li></ul><p>保存并测试</p><ul><li><p>保存文件，关闭编辑器。</p></li><li><p>激活环境：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">conda <span class="hljs-built_in">activate</span> liver_cancer<br></code></pre></td></tr></table></figure></li><li><p>启动Jupyter：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">jupyter notebook</span><br></code></pre></td></tr></table></figure></li><li><p>浏览器打开后，检查是否默认显示设置的目录。</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>下载hugging face上的语言模型并用Ollama部署</title>
      <link href="/%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B/"/>
      <url>/%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="下载模型"><a href="#下载模型" class="headerlink" title="下载模型"></a>下载模型</h2><p>首先打开<a href="https://huggingface.co/">HuggingFace</a>官网（需要科学上网），在首页上方点击<a href="https://huggingface.co/models">Models</a>，通过模型名称检索模型（以DeepSeek-R1-Distill-Qwen-7B为例），在模型检索框内输入检索的模型名称后按回车键检索，由于使用ollama部署，我们观察检索结果，选择结果中后缀带有<code>GGUF</code>字样的模型，点击查看，在跳转的页面中，清楚显示了模型作者分享的各种量化版本，选择合适的版本，点击名称进行下载，等待下载完即可。</p><p>在模型页面，作者通常附有如何选择对应模型的解释，一般来说按照以下的方式选择：</p><ol><li>获取本地电脑显存信息：打开任务管理器（<code>右键任务栏点击任务管理器，或使用快捷键Ctrl+Shift+ESC打开</code>），点击上方列表的性能，在左侧列表内点击GPU，可以看到GPU内存大小，其中有<code>专用GPU内存</code>大小和<code>共享GPU内存</code>大小，专用的代表GPU本身的内存大小，共享的代表由内存条额外共享的内存大小，总的GPU内存大小为这两者之和，如果想要生成速度尽可能快，仅关注专用GPU内存大小就行，如果希望牺牲一定的速度换取性能，可关注总的GPU内存大小。</li><li>选择量化大小：原则为选择文件大小约比GPU内存大小至少小1~2个G的量化版本。</li><li>选择量化类型：通常选择<code>K-quant</code>版本就行，如<code>QX_K_X</code>、<code>Q5_K_M</code>。</li></ol><h2 id="创建Ollama识别文件"><a href="#创建Ollama识别文件" class="headerlink" title="创建Ollama识别文件"></a>创建Ollama识别文件</h2><p>下载完成后，新建一个文本文件创建（如<code>deepseek-r1:32b-modelfile.txt</code>）ollama的可识别Modelfile文件，使用记事本编辑内容，将以下内容复制后保存：</p><figure class="highlight handlebars"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs handlebars"><span class="language-xml"># FROM deepseek-r1:14b （标记模型名称）</span><br><span class="language-xml"></span><br><span class="language-xml"># 从本地 GGUF 文件加载模型，路径替换为实际的文件路径，最好不要包含中文。</span><br><span class="language-xml"># 如果路径包含反斜杠 \，需用双反斜杠 \\ 或正斜杠 /表示。</span><br><span class="language-xml">FROM C:\Users\77177\.ollama\models\blobs\sha256-6e9f90f02bb3b39b59e81916e8cfce9deb45aeaeb9a54a5be4414486b907dc1e</span><br><span class="language-xml"></span><br><span class="language-xml"> # 设置对话模板（不用管）</span><br><span class="language-xml">TEMPLATE &quot;&quot;&quot;</span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> if .System &#125;&#125;</span><span class="hljs-template-variable">&#123;&#123; <span class="hljs-name">.System</span> &#125;&#125;</span><span class="hljs-template-variable">&#123;&#123; <span class="hljs-name">end</span> &#125;&#125;</span><span class="language-xml"></span><br><span class="language-xml"></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> range $i, $_ <span class="hljs-attr">:</span>= .Messages &#125;&#125;</span><span class="language-xml"></span><br><span class="language-xml"></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> $last <span class="hljs-attr">:</span>= eq (<span class="hljs-name">len</span> (<span class="hljs-name">slice</span> $.Messages $i)) <span class="hljs-number">1</span>&#125;&#125;</span><span class="language-xml"></span><br><span class="language-xml"></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> if eq .Role <span class="hljs-string">&quot;user&quot;</span> &#125;&#125;</span><span class="language-xml">&lt;｜User｜&gt;</span><span class="hljs-template-variable">&#123;&#123; <span class="hljs-name">.Content</span> &#125;&#125;</span><span class="language-xml"></span><br><span class="language-xml"></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> else if eq .Role <span class="hljs-string">&quot;assistant&quot;</span> &#125;&#125;</span><span class="language-xml">&lt;｜Assistant｜&gt;</span><span class="hljs-template-variable">&#123;&#123; <span class="hljs-name">.Content</span> &#125;&#125;</span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> if not $last &#125;&#125;</span><span class="language-xml">&lt;｜end▁of▁sentence｜&gt;</span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> end &#125;&#125;</span><span class="language-xml"></span><br><span class="language-xml"></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> end &#125;&#125;</span><span class="language-xml"></span><br><span class="language-xml"></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> if and $last (<span class="hljs-name">ne</span> .Role <span class="hljs-string">&quot;assistant&quot;</span>) &#125;&#125;</span><span class="language-xml">&lt;｜Assistant｜&gt;</span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> end &#125;&#125;</span><span class="language-xml"></span><br><span class="language-xml"></span><span class="hljs-template-variable">&#123;&#123;<span class="hljs-name">-</span> end &#125;&#125;</span><span class="language-xml">&quot;&quot;&quot;</span><br><span class="language-xml">PARAMETER stop &lt;｜begin▁of▁sentence｜&gt;</span><br><span class="language-xml">PARAMETER stop &lt;｜end▁of▁sentence｜&gt;</span><br><span class="language-xml">PARAMETER stop &lt;｜User｜&gt;</span><br><span class="language-xml">PARAMETER stop &lt;｜Assistant｜&gt;</span><br></code></pre></td></tr></table></figure><h2 id="通过命令行创建模型"><a href="#通过命令行创建模型" class="headerlink" title="通过命令行创建模型"></a>通过命令行创建模型</h2><ol><li><strong>以管理员身份打开 PowerShell</strong><br> 右键点击Windows 开始菜单 → 选择 **Windows PowerShell (**<strong>管理员)</strong>。</li><li><strong>执行创建命令</strong><br> 输入以下命令（替换你的 Modelfile 路径和模型名）：</li></ol><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nix">ollama create my-deepseek-r1 <span class="hljs-operator">-</span>f <span class="hljs-string">&quot;D:<span class="hljs-char escape_">\m</span>odels<span class="hljs-char escape_">\d</span>eepseek-r1-modelfile.txt&quot;</span><br></code></pre></td></tr></table></figure><ul><li><code>my-deepseek-r1</code>：自定义的模型名称（可任意命名，如 <code>deepseek-r1:32b-q6kl</code>）。</li><li><code>-f</code>：指定Modelfile文件的路径</li></ul><ol><li><p><strong>验证是否成功</strong><br>如果看到以下输出，表示模型已创建：</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">Successfully</span> created <span class="hljs-string">&#x27;my-deepseek-r1&#x27;</span><br></code></pre></td></tr></table></figure></li><li><p><strong>运行模型</strong></p></li></ol><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">ollama <span class="hljs-built_in">run</span> <span class="hljs-keyword">my</span>-deepseek-r1<br></code></pre></td></tr></table></figure><ul><li>输入测试问题（如 <code>你好，你能做什么？</code>），观察模型是否正常响应。</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2025年五一去汕头旅行</title>
      <link href="/2025%E5%B9%B4%E4%BA%94%E4%B8%80%E5%8E%BB%E6%B1%95%E5%A4%B4%E6%97%85%E8%A1%8C/"/>
      <url>/2025%E5%B9%B4%E4%BA%94%E4%B8%80%E5%8E%BB%E6%B1%95%E5%A4%B4%E6%97%85%E8%A1%8C/</url>
      
        <content type="html"><![CDATA[<p>2025年，我们商量今年五一沐宸来我这边。</p><p>关于五一，我们应该要去哪玩呢，我们临时想到可以去汕头，又或者是香港麦理浩径徒步。于是我们开始在网上寻找各种攻略，看汕头有什么好吃的，香港麦理浩径的路线怎么走。最终我们先</p>]]></content>
      
      
      
        <tags>
            
            <tag> 旅行 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调用API使用大语言模型教程</title>
      <link href="/%E8%B0%83%E7%94%A8API%E4%BD%BF%E7%94%A8%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E7%A8%8B/"/>
      <url>/%E8%B0%83%E7%94%A8API%E4%BD%BF%E7%94%A8%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="调用API使用大语言模型教程"><a href="#调用API使用大语言模型教程" class="headerlink" title="调用API使用大语言模型教程"></a>调用API使用大语言模型教程</h1><h2 id="一、获取API-Key"><a href="#一、获取API-Key" class="headerlink" title="一、获取API Key"></a>一、获取API Key</h2><p>登录各大商家的大模型开发平台，以字节火山API为例，打开<a href="https://console.volcengine.com/">火山方舟</a>网页：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223135308121.png" alt="image-20250223135308121"></p><p>​直接选择<strong>手机号登录</strong>，输入手机号获取验证码后登录到<strong>火山引擎控制台</strong>，点击右上角的图标展开栏目，在弹出来的列表里往下滚找到<strong>人工智能与算法板块</strong>，选择机器学习栏目下的<strong>火山方舟</strong>，点击后进入火山方舟控制台，右侧的小图钉图标可以固定，方便下次登录后直接点击：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223135457486.png" alt="image-20250223135457486"></p><p>进入后会先弹出跨服授权，我们点击<strong>同意授权</strong>：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223135819575.png" alt="image-20250223135819575"></p><p>随后点击左侧下方的<strong>API Key管理</strong>，如果没见到就先点击左下角的展开栏目图标：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223140207680.png" alt="image-20250223140207680"></p><p>打开后我们选择<strong>创建API</strong>，名字可以不用管，这里我们可以简单起一个CherryStudio，创建后点击这边的<strong>小眼睛图标</strong>，将显示的<strong>API Key复制</strong>下来，这样API Key就获取成功了：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223140237313.png" alt="image-20250223140237313"><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223140412798.png" alt="image-20250223140412798"></p><h2 id="二、获取模型信息"><a href="#二、获取模型信息" class="headerlink" title="二、获取模型信息"></a>二、获取模型信息</h2><p>点击左上方的模型广场，在搜索栏内输入deepseek搜索，点击查看DeepSeek-R1模型的查看详情：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223142334319.png" alt="image-20250223142334319"></p><p>在弹出来的详细信息中，选择DeepSeek-R1模型，复制下方的r1模型的Model ID：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223142453047.png" alt="image-20250223142453047"></p><h2 id="三、下载本地调用软件"><a href="#三、下载本地调用软件" class="headerlink" title="三、下载本地调用软件"></a>三、下载本地调用软件</h2><p>本地调用的软件很多，如CherryStudio、ChatBox、AnythingLLM等，这里我们以CherryStudio为例，点击进入CherryStudio的<a href="https://cherry-ai.com/download">下载页面</a>，系统会自动识别应该下载安装包，点击立即下载：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223140757864.png" alt="image-20250223140757864"></p><p>下载完后进行安装，如果有提示，不用管它，点击<u>更多信息</u>，选择仍要运行，然后一切按照默认安装，或者根据自己需求修改安装位置：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223140933128.png" alt="image-20250223140933128"></p><h2 id="四、软件内配置模型"><a href="#四、软件内配置模型" class="headerlink" title="四、软件内配置模型"></a>四、软件内配置模型</h2><p>安装好后，打开软件：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223141044061.png" alt="image-20250223141044061"></p><p>点击左下角的齿轮（设置图标），在模型服务栏目往下滑找到火山引擎，点击后进入配置页面：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223141210991.png" alt="image-20250223141210991"></p><p>将刚刚从火山方舟上生成的API Key复制到API 密钥框，在API 地址框内输入<code>https://ark.cn-beijing.volces.com/api/v3/</code>，随后点击添加，将我们刚刚复制的Model ID输入模型 ID栏目，模型名称与分组可以自己修改：</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223141501248.png" alt="image-20250223141501248"></p><p>配置好后，点击上方API 密钥框右侧的检查，选择刚刚配置的模型，确定进行检查，检查无误后，检查按钮会变为绿色的✔，随后点击软件右上角的对话图标，再点击上方的笔记图标快速开启话题，点击模型名称，在弹出的下拉选项中选择我们刚刚配置的模型，就可以开启对话啦。</p><p><img src="C:\Users\77177\AppData\Roaming\Typora\typora-user-images\image-20250223142002124.png" alt="image-20250223142002124"></p><p>备注：火山引擎默认赠送50万Tokens，约可支持200~400次对话。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 教程 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
